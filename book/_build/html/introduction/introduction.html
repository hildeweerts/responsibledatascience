
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Responsible Data Science &#8212; An Introduction to Responsible Data Science</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Machine Learning Preliminaries" href="machinelearning/introduction.html" />
    <link rel="prev" title="An Introduction to Responsible Data Science" href="../index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <h1 class="site-logo" id="site-title">An Introduction to Responsible Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   An Introduction to Responsible Data Science
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Responsible Data Science
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="machinelearning/introduction.html">
   Machine Learning Preliminaries
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="machinelearning/machinelearning.html">
     Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machinelearning/modelevaluation.html">
     Model Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machinelearning/modelselection.html">
     Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="machinelearning/costsensitivelearning.html">
     Cost-Sensitive Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ethics/introduction.html">
   Ethics Preliminaries
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Fairness
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../fairness/introduction.html">
   Algorithmic Fairness
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../fairness/groupfairness/introduction.html">
   Group Fairness
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/groupfairness/metrics.html">
     Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/groupfairness/mitigation.html">
     Mitigation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../fairness/individualfairness/introduction.html">
   Individual Fairness
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/individualfairness/metrics.html">
     Individual Fairness Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/individualfairness/mitigation.html">
     Mitigation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../fairness/counterfactualfairness/introduction.html">
   Counterfactual Fairness
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/counterfactualfairness/metrics.html">
     Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/counterfactualfairness/mitigation.html">
     Mitigation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../fairness/casestudies/introduction.html">
   Case Studies
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../fairness/casestudies/compas/introduction.html">
     COMPAS
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Explainability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../explainability/introduction.html">
   Explainable Artificial Intelligence
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Accountability
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../accountability/introduction.html">
   Algorithmic Accountability and Auditing
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Misc
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../other/glossary.html">
   Glossary
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/introduction/introduction.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/hildeweerts/responsibledatascience"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/hildeweerts/responsibledatascience/issues/new?title=Issue%20on%20page%20%2Fintroduction/introduction.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-responsible-data-science">
   What is
   <em>
    Responsible Data Science
   </em>
   ?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fairness">
     Fairness
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#transparency">
     Transparency
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accountability">
     Accountability
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#for-who-is-this-book">
   For who is this book?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-covered-in-this-book">
   What is covered in this book?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#navigating-this-book">
   Navigating this book
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="responsible-data-science">
<h1>Responsible Data Science<a class="headerlink" href="#responsible-data-science" title="Permalink to this headline">¶</a></h1>
<p>With the advent of large-scale data collection, the toolkit of a data scientist has proven to be a powerful way to make products and processes faster, cheaper, and better. Many data science applications make use of <a class="reference internal" href="../other/glossary.html#term-machine-learning"><span class="xref std std-term">machine learning</span></a> algorithms: algorithms that build mathematical models by `learning’ from data. Nowadays, machine learning models are integrated in many computer systems: from music recommendations to automated fraud detection, facial recognition systems, and personalized medicine assistants. These systems can provide benefits, but are not without risks.</p>
<p>A responsible data scientist understands how machine learning models might be harmful and how the risks can be mitigated. This online book provides a practical introduction to the nascent field of responsible machine learning.</p>
<div class="section" id="what-is-responsible-data-science">
<h2>What is <em>Responsible Data Science</em>?<a class="headerlink" href="#what-is-responsible-data-science" title="Permalink to this headline">¶</a></h2>
<p>So what is a ‘responsible’ approach? Generally speaking, responsibility is a <em>‘duty or obligation to take care of something’</em>. Taking responsibility involves actively avoiding that something ‘bad’ happens or increasing the probability that something ‘good’ happens. What can be considered ‘right’ or ‘wrong’ is the central question of ethics and has occupied philosophers for many centuries.</p>
<p>As machine learning systems are increasingly deployed in all kinds of applications, several incidents have shown in what ways machine learning systems can have negative consequences.</p>
<p>Machine learning models can inherit existing prejudices embedded in society, which can result in discrimination. The increasing complexity of machine learning systems can lead to opaque decision-making systems. And the increasing power of the organizations who deploy these systems raises questions about accountability.</p>
<p>I have organized these lecture notes along three key moral values: fairness, transparency, and accountability. Although there exists some overlap, each theme emphasizes a different aspect of a responsible approach to machine learning.</p>
<div class="section" id="fairness">
<h3>Fairness<a class="headerlink" href="#fairness" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../other/glossary.html#term-fairness"><span class="xref std std-term">Fairness</span></a> is a moral value that concerns treatment of behavior that is <em>just</em> and <em>free from discrimination</em>. Machine learning models, particularly classifiers, are specifically designed to discriminate between cases. As with any decision-making process, these distinctions can be undesirable from an ethical perspective or unlawful if they disproportionately affect people on the basis of sensitive characteristics such as gender, race, religion, age, and sexual orientation.</p>
<p>In recent years, several incidents have shown that machine learning systems can inherit and amplify social biases embedded in society. In 2016, investigative journalists from Propublica found that COMPAS, a decision support tool for assessing the likelihood of a defendant becoming a recidivist, wrongly labeled African-American defendants as recidivists at much higher rates than white Americans <span id="id1"><sup><a class="reference internal" href="#id4">AL16</a></sup></span>. Although concerns regarding the fairness of algorithmic decision-making systems is not new, Propublica’s article sparked an increased interest in the field.</p>
<p>Although math and numbers may seem objective, machine learning models are not value neutral. They are the result of many design choices that embed the values of their developers: which data is collected, what metrics are used to evaluate our models, and what problems do we decide to tackle in the first place? Consequently, the risk of unfairness is not limited to nefarious actors - even a well-intentioned data scientist has their blind spots.</p>
</div>
<div class="section" id="transparency">
<h3>Transparency<a class="headerlink" href="#transparency" title="Permalink to this headline">¶</a></h3>
<p>As a moral value, <a class="reference internal" href="../other/glossary.html#term-transparency"><span class="xref std std-term">transparency</span></a> can be defined as the <em>degree of openness that allows others to understand what actions are performed</em>. In the context of machine learning, an important dimension of transparency is the extent to which we can understand a model’s prediction-generating process. This is known as <a class="reference internal" href="../other/glossary.html#term-interpretable-machine-learning"><span class="xref std std-term">interpretable machine learning</span></a> or <a class="reference internal" href="../other/glossary.html#term-explainable-machine-learning"><span class="xref std std-term">explainable machine learning</span></a>.</p>
<p>In some cases, the best performing models are complex models such as deep neural networks and ensembles. As the complexity of models increases, it generally becomes more difficult for humans to understand their behavior. In many contexts, it can be valuable or even imperative to understand why a machine learning model makes certain predictions. For example, machine learning practitioners might use explanations to understand where the model fails and how it might be improved. Moreover, in regulated fields such as banking and health care, the ability to justify decisions is often mandated by law. In this case, explainability can be closely related to accountability.</p>
</div>
<div class="section" id="accountability">
<h3>Accountability<a class="headerlink" href="#accountability" title="Permalink to this headline">¶</a></h3>
<p>Previously, I have defined responsibility as a duty to take care of something. Responsibility can also be defined as being <em>accountable</em> for something. <a class="reference internal" href="../other/glossary.html#term-accountability"><span class="xref std std-term">Accountability</span></a> considers being held responsible for one’s actions, typically after something `bad’ has happened. Due to the apparent complexity of algorithmic systems, organizations may try to divert blame to the algorithm: <em>“oh, it’s just the algorithm.”</em> Algorithmic accountability is the idea that an institution should be held accountable for the use, design, and decisions of an algorithmic system. It involves taking adequate measures to comply with ethical principles or legal regulations, including detailed documentation and clear procedures for appealing decisions.</p>
<p>An important tool for fostering accountability is auditing, in which the development process, usage, and impact of an algorithmic system are closely inspected. This can be done either through internal procedures or by an external third party.</p>
</div>
</div>
<div class="section" id="for-who-is-this-book">
<h2>For who is this book?<a class="headerlink" href="#for-who-is-this-book" title="Permalink to this headline">¶</a></h2>
<p>The intended audience of this book are (undergraduate) data science students, practitioners, and researchers interested in responsible data science. It is expected that you are familiar with machine learning in general and supervised machine learning in particular. This includes the high-level workings of basic classification algorithms, model selection approaches, and evaluation metrics. Additionally, I expect readers to have some basic knowledge on normative ethical frameworks and terminology. To get you up to speed, I will go over some of the basic concepts in the chapter <a class="reference internal" href="machinelearning/introduction.html#mlpreliminaries"><span class="std std-ref">Machine Learning Preliminaries</span></a>.</p>
</div>
<div class="section" id="what-is-covered-in-this-book">
<h2>What is covered in this book?<a class="headerlink" href="#what-is-covered-in-this-book" title="Permalink to this headline">¶</a></h2>
<p>With the increasing usage of machine learning in real-world applications, I have also seen more examples of how machine learning can go wrong. As a result, the interest in a more responsible approach to data science has surged. At one end of the spectrum, the field has been productive in generating principles, guidelines, and frameworks for more ‘ethical’ artificial intelligence. However, many of these principles are too broad to guide the daily practice of a data scientist. At the other end of the spectrum, many technical solutions have been proposed. Although these efforts are laudable, these solutions often forego the real-world context of machine learning applications.</p>
<p>In this book, I have tried to balance these extremes by setting out a practical approach that acknowledges the sociotechnical context in which machine learning applications are used.</p>
<p>The techniques, challenges, and considerations discussed in these lecture notes mostly involve applications based on supervised learning. Nonetheless, the sociotechnical approach that is encouraged throughout the lecture notes can be applied to any analytics project.</p>
<p>Of course, there is much more to cover than what reasonably fits into one book. In addition to the values of fairness, explainability, and accountability, the EU High Level Expert Group on AI has identified additional key requirements of trustworthy artificial intelligence <span id="id2"><sup><a class="reference internal" href="#id5">IndependentEGoAIntelligence20</a></sup></span>: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, and (4) sustainability. Additionally, there are many adjacent topics such as moral philosophy, accessible product design, and organizational best practices. Although I wholeheartedly believe in the importance of organizational changes, including efforts such as ethics committees and programs for diversity and inclusion, these are not the main focus of these lecture notes.</p>
</div>
<div class="section" id="navigating-this-book">
<h2>Navigating this book<a class="headerlink" href="#navigating-this-book" title="Permalink to this headline">¶</a></h2>
<p>The rest of this book is organized as follows:</p>
<ul class="simple">
<li><p>Chapter <a class="reference internal" href="machinelearning/introduction.html#mlpreliminaries"><span class="std std-ref">Machine Learning</span></a> contains a quick recap of relevant concepts in machine learning.</p></li>
<li><p>Chapters <a class="reference internal" href="../fairness/groupfairness/introduction.html#groupfairness"><span class="std std-ref">Group Fairness</span></a>, <a class="reference internal" href="../fairness/individualfairness/introduction.html#individualfairness"><span class="std std-ref">Individual Fairness</span></a>, and <a class="reference internal" href="../fairness/counterfactualfairness/introduction.html#counterfactualfairness"><span class="std std-ref">Counterfactual Fairness</span></a> dive into the topic of algorithmic fairness, including techniques to discover and mitigate undesirable discrimination that have been proposed in the computer science literature.</p></li>
<li><p><a class="reference internal" href="../explainability/introduction.html#introxai"><span class="std std-ref">Explainable Machine Learning</span></a> covers several techniques for creating interpretable models and explaining black-box machine learning models.</p></li>
<li><p><a class="reference internal" href="../accountability/introduction.html#introaccountability"><span class="std std-ref">Algorithmic Accountability and Auditing</span></a> introduces algorithmic accountability and provides an introduction to internal and external audits of algorithmic decision-making systems.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Developing machine learning applications is not an objective task: it will require you to make many decisions, which often involves making trade-offs between competing criteria. I believe that taking responsibility requires one to make these decisions explicit. At times, this will require ethical decision-making. Given the great variety of norms and values that exist in the world, I would like to acknowledge that some of the ideas discussed in this book may reflect perspectives you do not agree with or values you do not personally consider important. The goal of this book is not to enforce a particular moral framework or political point-of-view. Instead, I aim to provide you with the knowledge and tools to identify trade-offs and make substantiated decisions.</p>
<p>This book summarizes many of the things I have learned so far on my journey in the field of responsible data science. Nevertheless, I am also but a person, with my own limited perspective. In particular, I would like to acknowledge that as an able-bodied white women that was born and raised in the Netherlands, I lack lived experience that is the daily reality of many people in marginalized communities. Additionally, my background is primarily technical and I am still learning from all great scholars in this interdisciplinary field. I hope you will too.</p>
</div>
<p id="id3"><dl class="citation">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id1">AL16</a></span></dt>
<dd><p>Julia Angwin and Jeff Larson. Machine Bias. 2016. URL: <a class="reference external" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>.</p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id2">IndependentEGoAIntelligence20</a></span></dt>
<dd><p>Independent Expert Group on Artificial Intelligence. The Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self assessment. 2020. URL: <a class="reference external" href="https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=68342">https://ec.europa.eu/newsroom/dae/document.cfm?doc_id=68342</a>.</p>
</dd>
</dl>
</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./introduction"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../index.html" title="previous page">An Introduction to Responsible Data Science</a>
    <a class='right-next' id="next-link" href="machinelearning/introduction.html" title="next page">Machine Learning Preliminaries</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Hilde Weerts<br/>
        
            &copy; Copyright 2022.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>